### 网络优化

Mini-Batch Gradient Descent

影响小批量梯度下降的方法因素有：批量大小K、学习率$\alpha$以及参数更新方向

选择合适的批量大小、调整学习率以及参数更新方向是神经网络优化的三个方向 



#### 批量大小选择：

批量大小不影响随机梯度的期望，但是会影响随机梯度的方差；

批量大小越大，随机梯度的方差越小，引入的噪声就越小，训练越稳定，可以设置较大的学习率

批量大小越小，需要设置较小的学习率，否则模型不收敛

学习率通常要跟着批量大小的增大而相应的增大

线性缩放规则--批量大小增加m倍时，学习率也增加m倍



#### 学习率调整

学习率是神经网络优化时的重要超参数。在梯度下降法中，学习率$\alpha$的取值非常关键，如果过大就不会收敛，如果过小就收敛速度太慢。常用的学习率调整方法包括学习率衰减、学习率预热、周期学习率以及一些自适应调整学习率的方法比如AdaGrad、RMSprop、AdaDelta

##### 学习率衰减

从经验上看，学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。比较简单的方式可以通过学习率衰减或者叫做学习率退火来完成。

常见的衰减方法如下图：fixed、step、exp、inv、multistep、poly

![1566553710382](C:\Users\Admin\AppData\Roaming\Typora\typora-user-images\1566553710382.png)



##### 学习率预热：

小批量梯度下降法中，当批量大小的设置比较大时，通常需要比较大的学习率，但在刚开始训练时，由于参数是随机初始化的，梯度往往会比较大，再加上比较大的学习率，会让训练不稳定

为了提高训练稳定性，可以在最初几轮迭代时，采用比较小的学习率，等梯度下降到一定程度后再恢复到初始的学习率，这种方法称为学习率预热

预热过程结束后，再选择一种学习率衰减方法来逐渐降低学习率



##### 周期性学习率调整：

为了使梯度下降方法能够逃离局部最小值或者鞍点，一种经验性的方式是在训练过程中周期性地增大学习率，虽然增大学习率可能短期内有损网络的收敛稳定性，但从长期来看有助于找到更好的局部最优解，周期性学习率调整满足这一点

循环学习率、带热重启的随机梯度下降



##### AdaGrad

核心思想是历史梯度平方和，可以理解为对周围环境的感知
$$
G_t=\sum_{\tau=1}^{t}g_\tau\odot g_\tau
$$

$$
\Delta\theta_t=-\frac{\alpha}{\sqrt{G_t+\epsilon}}\odot\vec{g_\tau}
$$


$g_\tau$表示第$\tau$次迭代时的梯度，$\odot$表示按照元素乘积

效果：

- 在凸优化中，`AdaGrad`算法效果较好。

- `AdaGrad`算法在某些深度学习模型上效果不错，但大多数情况下效果不好。

  在训练深度神经网络模型的实践中发现：`AdaGrad` 学习速率减小的时机过早，且减小的幅度过量。因为学习率与历史梯度的平方和的开方成反比，导致过快、过多的下降。



##### RMSprop

改进了AdaGrad算法中学习率不断单调下降以至过早衰减的缺点
$$
G_t=(1-\beta)\sum_{\tau=1}^{t}\beta^{t-\tau} \vec{g}_{\tau} \odot \vec{g}_{\tau}
$$

$$
\Delta \theta = -\frac{\alpha}{\sqrt{G_t+\epsilon}}\odot \vec{g_t}
$$

#### 更新方向优化



##### Momentum动量法

核心思想是从历史信息获得惯性

应用了学习率衰减的SGD算法的问题：有时候学习率会很小，但是明明可以应用一个更大的学习率

补充$$\Delta \theta_t=\theta_t -\theta_{t-1}$$


$$
\vec{v_t}=\Delta \theta=\rho \Delta\theta_{t-1}-\alpha \nabla_{\vec{\theta}} J(\vec{\theta})=\rho \vec{v_{t-1}}-\alpha \nabla_{\vec{\theta}} J(\vec{\theta})=\Delta \theta=\rho \Delta\theta_{t-1}-\alpha\vec{g_t}
$$

$$
\Delta \theta=\theta_t-\theta_{t-1}=\vec{v_t}
$$
学习率 $\alpha$

动量参数$\gamma$

参数$\theta$表示位移

动量方法是一种框架，可以应用到SGD中

动量法引入了变量$\vec{v}$充当速度的角色：刻画了参数在参数空间移动的方向和速度

超参数$\gamma[0,1)$描述了衰减权重的底数，$\gamma$越大，早期的梯度对当前的更新方向的影响越大



##### Nesterov动量法

Nesterov加速梯度（NAG），Nesterov动量法,计算梯度时采用新参数$\vec{\theta}+\alpha \vec{v}$
$$
（1）\vec{v_t}=\Delta \theta=\rho \Delta\theta_{t-1}-\alpha \nabla_{\vec{\theta}} J(\vec{\theta})=\rho \vec{v_{t-1}}-\alpha \nabla_{\vec{\theta}} J(\vec{\theta}+\rho \vec{v_{t-1}})
$$

$$
（2）\Delta \theta=\theta_t-\theta_{t-1}=\vec{v_t}
$$

但此方法不现实，

原因：过程复杂，step有5步如下

用现有的v去更新$\theta$

计算gradient

更新v

undo更新$\theta$

用更新过的v去更新$\theta$



真实生活中使用的公式如下：
$$
（3）\vec{v_t}=\Delta \theta=\gamma \Delta\theta_{t-1}-\alpha \nabla_{\vec{\theta_{t-1}}} J(\vec{\theta_{t-1}})=\gamma \vec{v_{t-1}}-\alpha \nabla_{\vec{\theta_{t-1}}} J(\vec{\theta_{t-1}})
$$

$$
（4）\Delta \theta=\theta_t-\theta_{t-1}=\gamma \vec{v_t}-\alpha \nabla_{\vec{\theta_{t-1}}} J(\vec{\theta_{t-1}})
$$

step变为3步：

计算梯度、更新v、更新$\theta$



经过数学变换（1）（2）可成为（3）（4）



##### Adam(Adaptive Moment Estimation ,Adam)自适应动量估计算法$\approx$动量法+RMSprop

结合惯性保持和环境感知两个优点

应用最广、比动量法快、但动量法最终结果往往好于Adam、有趋势先使用Adam再使用动量法训练、研究趋势是把Adam和动量法结合

$$
M_t=\beta_1M_{t-1}+(1-\beta_1)\vec{g_t}
$$

$$
G_t=\beta_2G_{t-1}+(1-\beta_2)\vec{g_t}\odot \vec{g_t}
$$

$$
\hat{M_t}=\frac{M_t}{1-\beta_1^t}
$$

$$
\hat G_t=\frac{G_t}{1-\beta_2^t}
$$

$$
\Delta \theta_t = -\frac{\alpha}{\sqrt{\hat G_t+\epsilon}}\hat M_t
$$



其中3、4公式是偏差修正项







